{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16059712-f7a8-42cb-9f60-65b4ed941621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "app_path = \"../data/application_record.csv\"   \n",
    "cred_path = \"../data/credit_record.csv\"\n",
    "\n",
    "app = pd.read_csv(app_path)\n",
    "cred = pd.read_csv(cred_path)\n",
    "\n",
    "print(\"application_record shape:\", app.shape)\n",
    "print(\"credit_record shape:\", cred.shape)\n",
    "\n",
    "\n",
    "display(app.head())\n",
    "display(cred.head())\n",
    "\n",
    "\n",
    "print(\"\\n--- application_record info ---\")\n",
    "display(app.info())\n",
    "display(app.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "print(\"\\n--- credit_record info ---\")\n",
    "display(cred.info())\n",
    "display(cred.isna().sum())\n",
    "\n",
    "\n",
    "def status_to_int(s):\n",
    "    if s in ['C','X'] or pd.isna(s):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "cred['STATUS_NUM'] = cred['STATUS'].apply(status_to_int)\n",
    "\n",
    "\n",
    "agg = cred.groupby('ID').agg(\n",
    "    num_records = ('STATUS', 'size'),\n",
    "    num_delinq = (lambda x: sum(x.isin(['2','3','4','5'])), 'sum'), \n",
    ").reset_index()\n",
    "\n",
    "\n",
    "grp = cred.groupby('ID')\n",
    "agg = pd.DataFrame({\n",
    "    'ID': grp.size().index,\n",
    "    'num_records': grp.size().values,\n",
    "    'num_delinq': grp.apply(lambda g: g['STATUS'].isin(['2','3','4','5']).sum()).values,\n",
    "    'num_missed': grp.apply(lambda g: g['STATUS'].isin(['1','2','3','4','5']).sum()).values,\n",
    "    'num_closed': grp.apply(lambda g: (g['STATUS']=='C').sum()).values,\n",
    "    'num_no_loan': grp.apply(lambda g: (g['STATUS']=='X').sum()).values,\n",
    "    'max_status': grp.apply(lambda g: pd.to_numeric(g['STATUS'], errors='coerce').max(skipna=True)).fillna(0).values,\n",
    "    'last_status': grp.apply(lambda g: g.sort_values('MONTHS_BALANCE')['STATUS'].iloc[0]).values,\n",
    "    'min_month': grp['MONTHS_BALANCE'].min().values,\n",
    "    'max_month': grp['MONTHS_BALANCE'].max().values,\n",
    "})\n",
    "\n",
    "\n",
    "agg['fraction_delinq'] = agg['num_delinq'] / agg['num_records']\n",
    "\n",
    "\n",
    "agg['risk_score'] = agg['num_delinq'] + 0.5*agg['num_missed']\n",
    "\n",
    "\n",
    "agg['high_risk'] = ((agg['num_delinq'] >= 2) | (agg['max_status'] >= 3) | (agg['fraction_delinq'] > 0.2)).astype(int)\n",
    "\n",
    "\n",
    "print(\"Target distribution (0 low-risk, 1 high-risk):\")\n",
    "display(agg['high_risk'].value_counts(normalize=False))\n",
    "display(agg['high_risk'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "merged = app.merge(agg, on='ID', how='inner')\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "display(merged.head())\n",
    "\n",
    "\n",
    "merged.to_csv(\"../data/merged_credit_data.csv\", index=False)\n",
    "print(\"Saved merged data to ../data/merged_credit_data.csv\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing values (merged):\")\n",
    "display(merged.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "\n",
    "counts = merged['high_risk'].value_counts()\n",
    "print(\"Class counts:\")\n",
    "print(counts)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "plt.xlabel(\"High risk (1) vs Low risk (0)\")\n",
    "plt.ylabel(\"Number of applicants\")\n",
    "plt.title(\"Class balance - initial target\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if 'OCCUPATION_TYPE' in merged.columns:\n",
    "    occ_tab = merged.groupby('OCCUPATION_TYPE')['high_risk'].mean().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nTop occupation types by mean high-risk rate (top 10):\")\n",
    "    display(occ_tab)\n",
    "\n",
    "\n",
    "numeric_cols = merged.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "for dropcol in ['ID','high_risk']:\n",
    "    if dropcol in numeric_cols:\n",
    "        numeric_cols.remove(dropcol)\n",
    "\n",
    "\n",
    "possible_cats = ['OCCUPATION_TYPE','FLAG_MOBIL','CODE_GENDER','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE']\n",
    "categorical_cols = [c for c in possible_cats if c in merged.columns]\n",
    "\n",
    "print(\"Numeric cols used:\", numeric_cols[:10])\n",
    "print(\"Categorical cols used:\", categorical_cols)\n",
    "\n",
    "\n",
    "numeric_subset = [c for c in ['AMT_INCOME_TOTAL','DAYS_EMPLOYED','DAYS_BIRTH','CNT_FAM_MEMBERS'] if c in merged.columns]\n",
    "print(\"Numeric subset for baseline:\", numeric_subset)\n",
    "\n",
    "\n",
    "X = merged[numeric_subset + categorical_cols].copy()\n",
    "y = merged['high_risk'].copy()\n",
    "\n",
    "\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.17647, stratify=y_train_val, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train class dist:\", y_train.value_counts(normalize=True).to_dict())\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_subset),\n",
    "    ('cat', cat_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "\n",
    "baseline = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', Perceptron(max_iter=1000, tol=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, split_name=\"set\"):\n",
    "    preds = model.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    prec = precision_score(y, preds, zero_division=0)\n",
    "    rec = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "    print(f\"\\nEvaluation on {split_name}:\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y, preds))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y, preds, zero_division=0))\n",
    "\n",
    "evaluate_model(baseline, X_train, y_train, \"TRAIN\")\n",
    "evaluate_model(baseline, X_val, y_val, \"VALIDATION\")\n",
    "evaluate_model(baseline, X_test, y_test, \"TEST\")\n",
    "\n",
    "\n",
    "joblib.dump(baseline, \"../models/perceptron_baseline.joblib\")\n",
    "print(\"Saved baseline model to ../models/perceptron_baseline.joblib\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
